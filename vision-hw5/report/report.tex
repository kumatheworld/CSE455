\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{caption}
\usepackage[subrefformat=parens,labelformat=parens]{subcaption}
\usepackage{graphicx}
\usepackage{cite}
\graphicspath{{figures/}}

\title{
  \vspace{-2cm}
  CSE 455 Homework 5 \\
  \author{Yoshihiro Kumazawa}
}

\begin{document}
\maketitle

\section{Installing PyTorch}
Done!

\section{Find the best network}
\subsection{Training a classifier using only one fully connected Layer}
See Figure~\ref{fig:2_1}. We can say that the the model successfully trained since the loss is decreasing throughout the training process and there is a healthy gap between the training accuracy and testing accuracy.

\subsection{Training a classifier using multiple fully connected Layers}
See Figure~\ref{fig:2_2}. The training is not successful because the testing accuracy plateaus whereas the training keeps increasing.

\subsubsection{Question}
See Figure~\ref{fig:2_2_1}. The model accuracy is significantly worse than the previous model. This is because the model is expressively limited since it has less non-linearity. The model can actually become just as good as LazyNet since without activations, the forward pass is just a couple of matrix multiplications, which is nothing but a single matrix multiplication by the composed matrices. But somehow, by separating the weight update process in back propagation, it achieves a slightly higher accuracy than LazyNet.

\subsection{Training a classifier using convolutions}
Our CoolNet is based on LeNet~\cite{Lecun98gradient-basedlearning}. In addition, batch normalization~\cite{pmlr-v37-ioffe15} is performed after each convolutional layer and linear layer except the final layer in order to get the network to converge faster. As a result, the network got higher accuracy then the previous models. See Figure~\ref{fig:2_3_bs4} for the training result.

\subsubsection{Question}
We tried batch sizes of 4, 32 and 256. See Figure~\ref{fig:2_3_bs4}, Figure~\ref{fig:2_3_bs32}, and Figure~\ref{fig:2_3_bs256} respectively. The loss curves account for the difference. Batch size 4 is so small to compute gradients accurately that the loss does not decrease sufficiently. On the other hand, when the batch size is 256, the total iteration number is much smaller and the training loss does not saturate. Among these 3 models, the model trained with batch size 32 performs best.
\begin{figure}
  \begin{subfigure}{0.16\textwidth}
    \centering
    \includegraphics[width=\linewidth]{accuracies_2_1.png}
  \end{subfigure}
  \begin{subfigure}{0.16\textwidth}
    \centering
    \includegraphics[width=\linewidth]{accuracies_2_2.png}
  \end{subfigure}
  \begin{subfigure}{0.16\textwidth}
    \centering
    \includegraphics[width=\linewidth]{accuracies_2_2_1.png}
  \end{subfigure}
  \begin{subfigure}{0.16\textwidth}
    \centering
    \includegraphics[width=\linewidth]{accuracies_2_3_bs4.png}
  \end{subfigure}
  \begin{subfigure}{0.16\textwidth}
    \centering
    \includegraphics[width=\linewidth]{accuracies_2_3_bs32.png}
  \end{subfigure}
  \begin{subfigure}{0.16\textwidth}
    \centering
    \includegraphics[width=\linewidth]{accuracies_2_3_bs256.png}
  \end{subfigure}

  \begin{subfigure}{0.16\textwidth}
    \centering
    \includegraphics[width=\linewidth]{loss_2_1.png}
    \caption{}
    \label{fig:2_1}
  \end{subfigure}
  \begin{subfigure}{0.16\textwidth}
    \centering
    \includegraphics[width=\linewidth]{loss_2_2.png}
    \caption{}
    \label{fig:2_2}
  \end{subfigure}
  \begin{subfigure}{0.16\textwidth}
    \centering
    \includegraphics[width=\linewidth]{loss_2_2_1.png}
    \caption{}
    \label{fig:2_2_1}
  \end{subfigure}
  \begin{subfigure}{0.16\textwidth}
    \centering
    \includegraphics[width=\linewidth]{loss_2_3_bs4.png}
    \caption{}
    \label{fig:2_3_bs4}
  \end{subfigure}
  \begin{subfigure}{0.16\textwidth}
    \centering
    \includegraphics[width=\linewidth]{loss_2_3_bs32.png}
    \caption{}
    \label{fig:2_3_bs32}
  \end{subfigure}
  \begin{subfigure}{0.16\textwidth}
    \centering
    \includegraphics[width=\linewidth]{loss_2_3_bs256.png}
    \caption{}
    \label{fig:2_3_bs256}
  \end{subfigure}
  \caption{Training results of LazyNet, BoringNet and CoolNet: \subref{fig:2_1} LazyNet, \subref{fig:2_2} BoringNet, \subref{fig:2_2_1} BoringNet without activations, \subref{fig:2_3_bs4} CoolNet with batch size 4, \subref{fig:2_3_bs32} CoolNet with batch size 32, \subref{fig:2_3_bs256} CoolNet with batch size 256.}
  \label{fig:2}
\end{figure}

\section{How does learning rate work?}
See Figure~\ref{fig:3_lr1e2} through \ref{fig:3_lr1e-4}. The model with learning rate 0.01 (Figure~\ref{fig:3_lr1e-2}) performs best. since its validation accuracy is the highest. Its training loss is the lowest too.

Figure~\ref{fig:3_e150} shows the training result of the model trained for 150 epochs. The training accuracy is better than the 50-epoch model (Figure~\ref{fig:3_lr1e-2}) but the testing accuracy is just as good. This is because the model is overfitting at an early stage of training, in which case training longer will not help.

\begin{figure}
  \begin{subfigure}{0.19\textwidth}
    \centering
    \includegraphics[width=\linewidth]{accuracies_3_lr1e2.png}
  \end{subfigure}
  \begin{subfigure}{0.19\textwidth}
    \centering
    \includegraphics[width=\linewidth]{accuracies_3_lr1e-1.png}
  \end{subfigure}
  \begin{subfigure}{0.19\textwidth}
    \centering
    \includegraphics[width=\linewidth]{accuracies_2_3_bs4.png}
  \end{subfigure}
  \begin{subfigure}{0.19\textwidth}
    \centering
    \includegraphics[width=\linewidth]{accuracies_3_lr1e-4.png}
  \end{subfigure}
  \begin{subfigure}{0.19\textwidth}
    \centering
    \includegraphics[width=\linewidth]{accuracies_3_e150.png}
  \end{subfigure}

  \begin{subfigure}{0.19\textwidth}
    \centering
    \includegraphics[width=\linewidth]{loss_3_lr1e2.png}
    \caption{}
    \label{fig:3_lr1e2}
  \end{subfigure}
  \begin{subfigure}{0.19\textwidth}
    \centering
    \includegraphics[width=\linewidth]{loss_3_lr1e-1.png}
    \caption{}
    \label{fig:3_lr1e-1}
  \end{subfigure}
  \begin{subfigure}{0.19\textwidth}
    \centering
    \includegraphics[width=\linewidth]{loss_2_3_bs4.png}
    \caption{}
    \label{fig:3_lr1e-2}
  \end{subfigure}
  \begin{subfigure}{0.19\textwidth}
    \centering
    \includegraphics[width=\linewidth]{loss_3_lr1e-4.png}
    \caption{}
    \label{fig:3_lr1e-4}
  \end{subfigure}
  \begin{subfigure}{0.19\textwidth}
    \centering
    \includegraphics[width=\linewidth]{loss_3_e150.png}
    \caption{}
    \label{fig:3_e150}
  \end{subfigure}
  \caption{Training results of CoolNet. \subref{fig:3_lr1e2} lr=10, \subref{fig:3_lr1e-1} lr=0.1, \subref{fig:3_lr1e-2} lr=0.01 (same as \ref{fig:2_3_bs4}), \subref{fig:3_lr1e-4} lr=0.0001, \subref{fig:3_e150} epoch=150. For \subref{fig:3_lr1e2}, the training loss was as high as 4.1 throughout the training process, which is so high that it does not show up in the figure.}
  \label{fig:3}
\end{figure}

\section{Data Augmentation}

\section{Change the loss function}

\bibliography{report}
\bibliographystyle{plain}
\end{document}
